"""
Disk Usage Analyzer - åˆ†æç£ç›˜ç©ºé—´ä½¿ç”¨æƒ…å†µ
"""
import os
import sys
import pickle
import hashlib
import json
import csv
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Optional, Set
from datetime import datetime
from platform_handler import get_platform_handler_singleton, FileInfo


class DiskAnalyzer:
    """ç£ç›˜ä½¿ç”¨åˆ†æå™¨"""

    def __init__(self, root_path: str):
        self.root_path = Path(root_path).resolve()
        self.total_size = 0
        self.file_count = 0
        self.dir_count = 0
        self.folders = {}  # path -> size
        self.file_types = defaultdict(int)  # extension -> size
        self.large_files = []  # list of (path, size)
        self.all_files = []  # è®°å½•æ‰€æœ‰æ–‡ä»¶ (path, size, mtime)
        self.scan_time = None  # æ‰«ææ—¶é—´
        self.cache_dir = Path('.analyzer_cache')

        # è·å–å¹³å°å¤„ç†å™¨å•ä¾‹
        self.platform = get_platform_handler_singleton()

        # ç»Ÿè®¡ä¿¡æ¯
        self.symlink_count = 0
        self.skipped_paths: Set[Path] = set()

    def scan(self, show_progress: bool = True):
        """æ‰«æç›®å½•"""
        if not self.root_path.exists():
            raise ValueError(f"è·¯å¾„ä¸å­˜åœ¨: {self.root_path}")

        print(f"ğŸ” æ­£åœ¨æ‰«æ: {self.root_path}")
        print("=" * 60)

        for root, dirs, files in os.walk(self.root_path):
            # è®¡ç®—å½“å‰ç›®å½•å¤§å°
            dir_size = 0

            # ä½¿ç”¨å¹³å°å¤„ç†å™¨è¿‡æ»¤éœ€è¦è·³è¿‡çš„ç›®å½•
            dirs[:] = [d for d in dirs if not self.platform.should_skip_path(Path(root) / d)]

            for file in files:
                file_path = Path(root) / file

                # ä½¿ç”¨å¹³å°å¤„ç†å™¨è·å–æ–‡ä»¶ä¿¡æ¯
                file_info: Optional[FileInfo] = self.platform.get_file_info(file_path)

                if file_info is None:
                    continue

                # ç»Ÿè®¡ç¬¦å·é“¾æ¥ï¼ˆä½†ä¸è®¡å…¥å¤§å°ï¼‰
                if file_info.is_symlink:
                    self.symlink_count += 1
                    # ç¬¦å·é“¾æ¥ä»ç„¶è®¡å…¥å¤§å°ï¼ˆæŒ‡å‘çš„æ–‡ä»¶å¤§å°ï¼‰
                    # å¦‚æœè¦è·³è¿‡ç¬¦å·é“¾æ¥ï¼Œå¯ä»¥åœ¨è¿™é‡Œ continue

                size = file_info.size
                mtime = file_info.mtime
                dir_size += size
                self.file_count += 1
                self.total_size += size

                # è®°å½•æ‰€æœ‰æ–‡ä»¶ï¼ˆç”¨äºé‡å¤æ£€æµ‹ï¼‰
                self.all_files.append((file_path, size, mtime))

                # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
                ext = file_path.suffix.lower() or "(æ— æ‰©å±•å)"
                self.file_types[ext] += size

                # è®°å½•å¤§æ–‡ä»¶ (> 100MB)
                if size > 100 * 1024 * 1024:
                    self.large_files.append((file_path, size))

            # è®°å½•æ–‡ä»¶å¤¹å¤§å°
            self.folders[Path(root)] = dir_size
            self.dir_count += 1

            # æ˜¾ç¤ºè¿›åº¦
            if show_progress and self.dir_count % 100 == 0:
                print(f"å·²æ‰«æ {self.dir_count} ä¸ªç›®å½•, {self.file_count} ä¸ªæ–‡ä»¶...", end='\r')

        if show_progress:
            print(f"\nâœ… æ‰«æå®Œæˆ! å…±æ‰«æ {self.dir_count} ä¸ªç›®å½•, {self.file_count} ä¸ªæ–‡ä»¶")
            if self.symlink_count > 0:
                print(f"ğŸ”— å‘ç° {self.symlink_count} ä¸ªç¬¦å·é“¾æ¥")
            if self.skipped_paths:
                print(f"â­ï¸  è·³è¿‡ {len(self.skipped_paths)} ä¸ªç‰¹æ®Šè·¯å¾„")

        self.scan_time = datetime.now()

    def get_top_folders(self, n: int = 10) -> List[Tuple[Path, int]]:
        """è·å–æœ€å¤§çš„nä¸ªæ–‡ä»¶å¤¹"""
        sorted_folders = sorted(self.folders.items(), key=lambda x: x[1], reverse=True)
        return sorted_folders[:n]

    def get_top_files(self, n: int = 10) -> List[Tuple[Path, int]]:
        """è·å–æœ€å¤§çš„nä¸ªæ–‡ä»¶"""
        return sorted(self.large_files, key=lambda x: x[1], reverse=True)[:n]

    def get_file_types_summary(self) -> Dict[str, int]:
        """è·å–æ–‡ä»¶ç±»å‹ç»Ÿè®¡"""
        return dict(sorted(self.file_types.items(), key=lambda x: x[1], reverse=True))

    @staticmethod
    def format_size(size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"

    # ==================== ç¼“å­˜åŠŸèƒ½ ====================

    def _get_cache_path(self) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # ä½¿ç”¨è·¯å¾„çš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜æ–‡ä»¶åï¼Œé¿å…è·¯å¾„ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        path_hash = hashlib.md5(str(self.root_path).encode('utf-8')).hexdigest()
        self.cache_dir.mkdir(exist_ok=True)
        return self.cache_dir / f"scan_{path_hash}.pkl"

    def save_cache(self):
        """ä¿å­˜æ‰«æç»“æœåˆ°ç¼“å­˜"""
        cache_data = {
            'root_path': str(self.root_path),
            'total_size': self.total_size,
            'file_count': self.file_count,
            'dir_count': self.dir_count,
            'folders': {str(k): v for k, v in self.folders.items()},
            'file_types': dict(self.file_types),
            'large_files': [(str(p), s) for p, s in self.large_files],
            'all_files': [(str(p), s, m) for p, s, m in self.all_files],
            'scan_time': self.scan_time,
        }

        cache_path = self._get_cache_path()
        with open(cache_path, 'wb') as f:
            pickle.dump(cache_data, f)

        return cache_path

    def load_cache(self) -> bool:
        """ä»ç¼“å­˜åŠ è½½æ‰«æç»“æœï¼Œè¿”å›æ˜¯å¦æˆåŠŸ"""
        cache_path = self._get_cache_path()

        if not cache_path.exists():
            return False

        try:
            with open(cache_path, 'rb') as f:
                cache_data = pickle.load(f)

            # éªŒè¯ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆæ£€æŸ¥æ ¹è·¯å¾„æ˜¯å¦åŒ¹é…ï¼‰
            if cache_data.get('root_path') != str(self.root_path):
                return False

            # æ¢å¤æ•°æ®
            self.total_size = cache_data['total_size']
            self.file_count = cache_data['file_count']
            self.dir_count = cache_data['dir_count']
            self.folders = {Path(k): v for k, v in cache_data['folders'].items()}
            self.file_types = defaultdict(int, cache_data['file_types'])
            self.large_files = [(Path(p), s) for p, s in cache_data['large_files']]
            self.all_files = [(Path(p), s, m) for p, s, m in cache_data['all_files']]
            self.scan_time = cache_data.get('scan_time')

            return True

        except Exception:
            return False

    def is_cache_valid(self, max_age_hours: int = 24) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆå­˜åœ¨ä¸”æœªè¿‡æœŸï¼‰"""
        cache_path = self._get_cache_path()

        if not cache_path.exists():
            return False

        # æ£€æŸ¥ç¼“å­˜å¹´é¾„
        cache_age = datetime.now().timestamp() - cache_path.stat().st_mtime
        if cache_age > max_age_hours * 3600:
            return False

        return True

    # ==================== é‡å¤æ–‡ä»¶æ£€æµ‹ ====================

    @staticmethod
    def _calculate_file_hash(file_path: Path, chunk_size: int = 8192) -> Optional[str]:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        try:
            md5 = hashlib.md5()
            with open(file_path, 'rb') as f:
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break
                    md5.update(chunk)
            return md5.hexdigest()
        except (PermissionError, FileNotFoundError):
            return None

    def find_duplicates(self, min_size: int = 1024 * 1024) -> Dict[str, List[Tuple[Path, int]]]:
        """
        æŸ¥æ‰¾é‡å¤æ–‡ä»¶

        Args:
            min_size: åªæ£€æµ‹å¤§äºæ­¤å¤§å°çš„æ–‡ä»¶ï¼ˆé»˜è®¤1MBï¼‰ï¼ŒåŠ å¿«é€Ÿåº¦

        Returns:
            {hash: [(file_path, size), ...]} - é‡å¤æ–‡ä»¶åˆ—è¡¨
        """
        # å¦‚æœæ”¯æŒinodeï¼Œä½¿ç”¨inodeå¿«é€Ÿå»é‡
        if self.platform.supports_inodes():
            return self._find_duplicates_with_inodes(min_size)
        else:
            return self._find_duplicates_by_hash(min_size)

    def _find_duplicates_with_inodes(self, min_size: int) -> Dict[str, List[Tuple[Path, int]]]:
        """
        ä½¿ç”¨inodeä¿¡æ¯å¿«é€ŸæŸ¥æ‰¾é‡å¤æ–‡ä»¶ï¼ˆLinux/macOSï¼‰

        é€šè¿‡inodeå¯ä»¥å¿«é€Ÿè¯†åˆ«ç¡¬é“¾æ¥ï¼Œé¿å…é‡å¤è®¡ç®—å“ˆå¸Œ
        """
        # æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„
        size_groups = defaultdict(list)
        inode_map: Dict[int, List[Path]] = defaultdict(list)

        for file_path, size, _ in self.all_files:
            if size < min_size:
                continue

            size_groups[size].append((file_path, size))

            # è·å–inodeä¿¡æ¯
            file_info = self.platform.get_file_info(file_path)
            if file_info and file_info.inode:
                inode_map[file_info.inode].append(file_path)

        # æ‰¾å‡ºç¡¬é“¾æ¥ï¼ˆç›¸åŒinodeçš„æ–‡ä»¶ï¼‰
        hardlink_groups = []
        for inode, paths in inode_map.items():
            if len(paths) > 1:
                # è¿™äº›æ˜¯ç¡¬é“¾æ¥ï¼Œåªä¿ç•™ä¸€ä¸ªç”¨äºå“ˆå¸Œè®¡ç®—
                hardlink_groups.append(paths[0])

        duplicates = {}
        checked_hashes = set()

        for size, files in size_groups.items():
            if len(files) < 2:
                continue

            # è®¡ç®—å“ˆå¸Œå€¼ï¼ˆè·³è¿‡ç¡¬é“¾æ¥ï¼Œåªè®¡ç®—ä¸€æ¬¡ï¼‰
            for file_path, size in files:
                # å¦‚æœæ˜¯ç¡¬é“¾æ¥çš„ä»£è¡¨æ–‡ä»¶ï¼Œæˆ–è€…ä¸åœ¨ç¡¬é“¾æ¥ç»„ä¸­
                is_representative = True
                for group in hardlink_groups:
                    if file_path in group and file_path != group[0]:
                        is_representative = False
                        break

                if not is_representative:
                    continue

                file_hash = self._calculate_file_hash(file_path)
                if file_hash and file_hash not in checked_hashes:
                    if file_hash not in duplicates:
                        duplicates[file_hash] = []
                    duplicates[file_hash].append((file_path, size))
                    checked_hashes.add(file_hash)

        # åªè¿”å›æœ‰é‡å¤çš„æ–‡ä»¶
        return {h: files for h, files in duplicates.items() if len(files) > 1}

    def _find_duplicates_by_hash(self, min_size: int) -> Dict[str, List[Tuple[Path, int]]]:
        """
        é€šè¿‡å“ˆå¸Œå€¼æŸ¥æ‰¾é‡å¤æ–‡ä»¶ï¼ˆWindowsï¼‰

        Args:
            min_size: åªæ£€æµ‹å¤§äºæ­¤å¤§å°çš„æ–‡ä»¶ï¼ˆé»˜è®¤1MBï¼‰ï¼ŒåŠ å¿«é€Ÿåº¦

        Returns:
            {hash: [(file_path, size), ...]} - é‡å¤æ–‡ä»¶åˆ—è¡¨
        """
        # æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„
        size_groups = defaultdict(list)
        for file_path, size, _ in self.all_files:
            if size >= min_size:
                size_groups[size].append((file_path, size))

        # åªå¤„ç†å¯èƒ½æœ‰é‡å¤çš„æ–‡ä»¶ç»„
        duplicates = {}
        checked_hashes = set()

        for size, files in size_groups.items():
            if len(files) < 2:
                continue  # æ²¡æœ‰é‡å¤çš„å¯èƒ½

            # è®¡ç®—å“ˆå¸Œå€¼
            for file_path, size in files:
                file_hash = self._calculate_file_hash(file_path)
                if file_hash and file_hash not in checked_hashes:
                    if file_hash not in duplicates:
                        duplicates[file_hash] = []
                    duplicates[file_hash].append((file_path, size))
                    checked_hashes.add(file_hash)

        # åªè¿”å›æœ‰é‡å¤çš„æ–‡ä»¶
        return {h: files for h, files in duplicates.items() if len(files) > 1}

    # ==================== å¯¼å‡ºåŠŸèƒ½ ====================

    def export_to_json(self, output_path: str):
        """å¯¼å‡ºæ‰«æç»“æœåˆ°JSONæ–‡ä»¶"""
        data = {
            'scan_info': {
                'root_path': str(self.root_path),
                'scan_time': self.scan_time.isoformat() if self.scan_time else None,
                'total_size': self.total_size,
                'file_count': self.file_count,
                'dir_count': self.dir_count,
            },
            'top_folders': [
                {'path': str(p), 'size': s, 'size_human': self.format_size(s)}
                for p, s in self.get_top_folders(20)
            ],
            'top_files': [
                {'path': str(p), 'size': s, 'size_human': self.format_size(s)}
                for p, s in self.get_top_files(20)
            ],
            'file_types': [
                {'extension': ext, 'size': s, 'size_human': self.format_size(s)}
                for ext, s in self.get_file_types_summary().items()
            ]
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        return output_path

    def export_to_csv(self, output_dir: str = '.'):
        """å¯¼å‡ºæ‰«æç»“æœåˆ°CSVæ–‡ä»¶"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        files_created = []

        # 1. æ–‡ä»¶å¤¹ç»Ÿè®¡
        folders_csv = output_path / f'folders_{timestamp}.csv'
        with open(folders_csv, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['è·¯å¾„', 'å¤§å°(å­—èŠ‚)', 'å¤§å°(å¯è¯»)', 'å æ¯”(%)'])

            for path, size in self.get_top_folders(100):
                percent = (size / self.total_size * 100) if self.total_size > 0 else 0
                writer.writerow([
                    str(path),
                    size,
                    self.format_size(size),
                    f'{percent:.2f}'
                ])
        files_created.append(folders_csv)

        # 2. æ–‡ä»¶ç±»å‹ç»Ÿè®¡
        types_csv = output_path / f'file_types_{timestamp}.csv'
        with open(types_csv, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['æ‰©å±•å', 'å¤§å°(å­—èŠ‚)', 'å¤§å°(å¯è¯»)', 'å æ¯”(%)'])

            for ext, size in self.get_file_types_summary().items():
                percent = (size / self.total_size * 100) if self.total_size > 0 else 0
                writer.writerow([
                    ext,
                    size,
                    self.format_size(size),
                    f'{percent:.2f}'
                ])
        files_created.append(types_csv)

        # 3. å¤§æ–‡ä»¶ç»Ÿè®¡
        files_csv = output_path / f'large_files_{timestamp}.csv'
        with open(files_csv, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['è·¯å¾„', 'å¤§å°(å­—èŠ‚)', 'å¤§å°(å¯è¯»)'])

            for path, size in self.get_top_files(100):
                writer.writerow([
                    str(path),
                    size,
                    self.format_size(size)
                ])
        files_created.append(files_csv)

        return files_created
